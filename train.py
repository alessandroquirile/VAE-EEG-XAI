import gc
import os
import pickle
from itertools import product

import numpy as np
import pandas as pd
import seaborn as sns
from keras.optimizers.legacy import Adam
from keras.src.callbacks import EarlyStopping
from matplotlib import pyplot as plt
from skimage.metrics import structural_similarity as ssim, mean_squared_error
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split, KFold
from tensorflow.python.ops.numpy_ops import np_config
from tqdm import tqdm

from models import *


def load_data(topomaps_folder: str, labels_folder: str, test_size, anomaly_detection):
    x, y = _create_dataset(topomaps_folder, labels_folder)

    print(f"Splitting data set into {1 - test_size} training set and {test_size} test set "
          f"{'for latent space analysis' if not anomaly_detection else 'for anomaly detection'}")

    seed = 42

    if anomaly_detection:
        # Training set only contains images whose label is 0
        train_indices = np.where(y == 0)[0]
        x_train = x[train_indices]
        y_train = y[train_indices]

        # Split the remaining data into testing sets
        remaining_indices = np.where(y != 0)[0]
        x_remaining = x[remaining_indices]
        y_remaining = y[remaining_indices]
        _, x_test, _, y_test = train_test_split(x_remaining, y_remaining, test_size=test_size, random_state=seed)

        # Check dataset for anomaly detection task
        y_train_only_contains_label_0 = all(y_train) == 0
        y_test_only_contains_label_1_and_2 = all(label in [0, 1, 2] for label in y_test)
        if not y_train_only_contains_label_0 or not y_test_only_contains_label_1_and_2:
            raise Exception("Data was not loaded successfully")
    else:
        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=seed)

    return x_train, x_test, y_train, y_test


def _create_dataset(topomaps_folder, labels_folder):
    topomaps_files = os.listdir(topomaps_folder)
    labels_files = os.listdir(labels_folder)

    topomaps_files.sort()
    labels_files.sort()

    x = []
    y = []

    n_files = len(topomaps_files)

    for topomaps_file, labels_file in tqdm(zip(topomaps_files, labels_files), total=n_files,
                                           desc=f"Loading data set from {topomaps_folder} and {labels_folder} folders"):
        if topomaps_file.endswith('.DS_Store'):
            continue
        topomaps_array = np.load(f"{topomaps_folder}/{topomaps_file}", allow_pickle=True)
        labels_array = np.load(f"{labels_folder}/{labels_file}", allow_pickle=True)
        if topomaps_array.shape[0] != labels_array.shape[0]:
            raise Exception("Shapes must be equal")
        for i in range(topomaps_array.shape[0]):
            x.append(topomaps_array[i])
            y.append(labels_array[i])

    x = np.array(x)
    y = np.array(y)

    return x, y


def scaled_ssim(original, reconstructed):
    # data_range=1 requires the data to be normalized between 0 and 1
    original = normalize(original)
    reconstructed = normalize(reconstructed)
    score = ssim(original, reconstructed, data_range=1, channel_axis=-1)
    return (score + 1) / 2  # The reference paper deals with ssim between 0 and 1 instead of -1 and 1


def plot_latent_space(vae, data, points_to_sample=30, figsize=15):
    """
    Plots the latent space of a Variational Autoencoder (VAE).
    This function generates a 2D manifold plot of digits in the latent space
    of the VAE. Each point in the plot represents a digit generated by the VAE's
    decoder model based on a specific location in the latent space.

    :param vae: The trained VAE model.
    :param data: Data to have a latent representation of. Shape should be (num_samples, 40, 40).
    :param points_to_sample: The number of points to sample along each axis of the plot. Default is 30.
    :param figsize: The size of the figure (width and height) in inches. Default is 15.
    :return: None (displays the plot).
    """

    np_config.enable_numpy_behavior()

    image_size = data.shape[1]
    scale = 1.0

    # Create an empty figure to store the generated images
    figure = np.zeros((image_size * points_to_sample, image_size * points_to_sample))

    # Define linearly spaced coordinates corresponding to the 2D plot in the latent space
    grid_x = np.linspace(-scale, scale, points_to_sample)
    grid_y = np.linspace(-scale, scale, points_to_sample)[::-1]  # Reverse the order of grid_y

    # Apply t-SNE to the latent space
    z_mean, _, _ = vae.encoder(data)
    tsne = TSNE(n_components=2, verbose=1)
    z_mean_reduced = tsne.fit_transform(z_mean)

    # Generate the images by iterating over the grid coordinates
    for i, yi in enumerate(grid_y):
        for j, xi in enumerate(grid_x):
            # Find the nearest t-SNE point to the current coordinates
            dist = np.sqrt((z_mean_reduced[:, 0] - xi) ** 2 + (z_mean_reduced[:, 1] - yi) ** 2)
            idx = np.argmin(dist)
            z_sample = z_mean[idx]

            # Decode the latent sample to generate an image
            x_decoded = vae.decoder(np.expand_dims(z_sample, axis=0))

            # Reshape the decoded image to match the desired image size
            digit = x_decoded.reshape(image_size, image_size)

            # Add the digit to the corresponding position in the figure
            figure[i * image_size: (i + 1) * image_size, j * image_size: (j + 1) * image_size] = digit

    # Plotting the figure
    plt.figure(figsize=(figsize, figsize))

    # Define the tick positions and labels for the x and y axes
    start_range = image_size // 2
    end_range = points_to_sample * image_size + start_range
    pixel_range = np.arange(start_range, end_range, image_size)
    sample_range_x = np.round(grid_x, 1)
    sample_range_y = np.round(grid_y, 1)
    plt.xticks(pixel_range, sample_range_x)
    plt.yticks(pixel_range, sample_range_y)

    # Set the x and y axis labels
    plt.xlabel("z[0]")
    plt.ylabel("z[1]")

    # Display the figure
    plt.imshow(figure)
    plt.show()


def plot_label_clusters(vae, data, labels):
    """
    Plots a t-SNE projection of the given data, with labels represented by different colors.

    :param vae: The trained VAE (Variational Autoencoder) model.
    :param data: Input data of shape (num_samples, 40, 40).
    :param labels: Array of labels corresponding to the data of shape (num_samples,).
    :return: None (displays the plot)
    """

    # call vs predict: https://stackoverflow.com/a/70205891/17082611
    z_mean, _, _ = vae.encoder(data)

    data = data.reshape(data.shape[0], -1)

    tsne = TSNE(n_components=2, verbose=1)
    z_mean_reduced = tsne.fit_transform(data)

    df = pd.DataFrame()
    df["labels"] = labels.flatten()
    df["comp-1"] = z_mean_reduced[:, 0]
    df["comp-2"] = z_mean_reduced[:, 1]

    # Get the distinct labels and the number of colors needed
    distinct_labels = np.unique(labels)
    n_colors = len(distinct_labels)

    # Create the plot
    fig = plt.figure(figsize=(10, 8))
    sns.scatterplot(data=df, x="comp-1", y="comp-2", hue=df.labels.tolist(),
                    palette=sns.color_palette("hls", n_colors)).set(title="Data t-SNE projection")
    # plt.show()
    return fig


def plot_metric(history, metric):
    # Legge dal pickle
    with open(history, "rb") as fp:
        history_data = pickle.load(fp)
    plt.plot(history_data[metric])
    plt.plot(history_data['val_' + metric])
    plt.title(metric)
    plt.ylabel(metric)
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'])
    plt.show()

    # Legge direttamente dalla history del fit
    """plt.plot(history.history[metric])
    plt.plot(history.history['val_' + metric])
    plt.title(metric)
    plt.ylabel(metric)
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'])
    plt.show()"""


def reduce_size(x, y, new_size):
    return x[:new_size], y[:new_size]


def expand(x):
    return np.expand_dims(x, -1).astype("float32")


def normalize(x):
    return (x - np.min(x)) / (np.max(x) - np.min(x))


def custom_grid_search(x_train, latent_dimension):
    param_grid = {
        'epochs': [2500],
        'l_rate': [10 ** -5, 10 ** -6, 10 ** -7],
        'batch_size': [32, 64, 128],
        'patience': [30]
    }

    print("\nlatent_dimension:", latent_dimension)
    print("Custom grid search with param_grid:", param_grid)

    grid_search = CustomGridSearchCV(param_grid)
    grid_search.fit(x_train, latent_dimension)

    return grid_search


def refit(fitted_grid, x_train, y_train, latent_dimension):
    print("\nRefitting based on best params:", fitted_grid.best_params_)

    best_epochs = fitted_grid.best_params_["epochs"]
    best_l_rate = fitted_grid.best_params_["l_rate"]
    best_batch_size = fitted_grid.best_params_["batch_size"]
    best_patience = fitted_grid.best_params_["patience"]

    val_size = 0.2
    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_size)
    print(f"validation data is {val_size} of training data")
    print("x_val shape:", x_val.shape)
    print("y_val shape:", y_val.shape)

    encoder = Encoder(latent_dimension)
    decoder = Decoder()
    vae = VAE(encoder, decoder, best_epochs, best_l_rate, best_batch_size, best_patience)
    vae.compile(Adam(best_l_rate))

    early_stopping = EarlyStopping("val_loss", patience=best_patience, verbose=1)

    history = vae.fit(x_train, x_train, best_batch_size, best_epochs,
                      validation_data=(x_val, x_val), callbacks=[early_stopping], verbose=1)

    # dbg
    cv = KFold(n_splits=3, shuffle=True, random_state=42)
    ssim_scores = []
    mse_scores = []
    for train_idx, val_idx in cv.split(x_train):
        x_train_fold, x_val_fold = x_train[train_idx], x_train[val_idx]
        predicted = vae.predict(x_val_fold)
        ssim = scaled_ssim(x_val_fold, predicted)
        mse = mean_squared_error(x_val_fold, predicted)
        ssim_scores.append(ssim)
        mse_scores.append(mse)
    avg_ssim = np.mean(ssim_scores)
    avg_mse = np.mean(mse_scores)
    avg_score = (avg_ssim + avg_mse) / 2
    print(f"[dbg] avg_ssim for best combination after fit: {avg_ssim:.4f}")
    print(f"[dbg] avg_mse for best combination after fit: {avg_mse:.4f}")
    print(f"[dbg] avg_score for best combination after fit: {avg_score:.4f}")

    return history, vae


def reconstruction_skill(vae, x_test):
    image_index = 5
    original_image = x_test[image_index]
    x_test_reconstructed = vae.predict(x_test, verbose=0)
    reconstructed_image = x_test_reconstructed[image_index]
    # ssim = my_ssim(original_image, reconstructed_image)
    return original_image, reconstructed_image


class CustomGridSearchCV:
    def __init__(self, param_grid):
        self.param_grid = param_grid
        self.best_params_ = {}
        self.best_score_ = None
        self.grid_ = []

    def fit(self, x_train, latent_dimension):
        param_combinations = list(product(*self.param_grid.values()))
        n_combinations = len(param_combinations)

        n_splits = 3
        print("n_splits:", n_splits)
        print(f"scorers: {scaled_ssim} and {mean_squared_error}")

        for params in tqdm(param_combinations, total=n_combinations, desc="Combination", unit="combination"):
            params_dict = dict(zip(self.param_grid.keys(), params))

            cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)
            ssim_scores = []
            mse_scores = []

            for train_idx, val_idx in cv.split(x_train):
                x_train_fold, x_val_fold = x_train[train_idx], x_train[val_idx]

                encoder = Encoder(latent_dimension)
                decoder = Decoder()
                vae = VAE(encoder, decoder, params_dict['epochs'], params_dict['l_rate'], params_dict['batch_size'])
                vae.compile(Adam(params_dict['l_rate']))

                early_stopping = EarlyStopping("val_loss", patience=params_dict['patience'])
                vae.fit(x_train_fold, x_train_fold, params_dict['batch_size'], params_dict['epochs'],
                        validation_data=(x_val_fold, x_val_fold), callbacks=[early_stopping], verbose=0)

                predicted = vae.predict(x_val_fold)
                ssim = scaled_ssim(x_val_fold, predicted)
                mse = mean_squared_error(x_val_fold, predicted)
                ssim_scores.append(ssim)
                mse_scores.append(mse)

                # Clear the TensorFlow session to free GPU memory
                # https://stackoverflow.com/a/52354943/17082611
                tf.keras.backend.clear_session()
                del encoder, decoder, vae
                gc.collect()

            avg_ssim = np.mean(ssim_scores)
            avg_mse = np.mean(mse_scores)
            avg_score = (avg_ssim + avg_mse) / 2

            params_dict['avg_score'] = avg_score
            params_dict['ssim'] = avg_ssim
            params_dict['mse'] = avg_mse
            self.grid_.append(params_dict)

            print(f"avg_ssim for current combination: {avg_score:.4f}")
            print(f"avg_mse for current combination: {avg_mse:.4f}")
            print(f"avg_score for current combination: {avg_score:.4f}")

            # Update the best hyperparameters based on the highest avg_score
            if self.best_score_ is None or avg_score > self.best_score_:
                self.best_score_ = avg_score
                self.best_params_ = params_dict

        return self


def save(history, subject):
    with open(f'history_{subject}.pickle', 'wb') as file_pi:
        pickle.dump(history.history, file_pi)


if __name__ == '__main__':
    print("TensorFlow GPU usage:", tf.config.list_physical_devices('GPU'))

    # Dati ridotti al solo intorno del blink
    subject = "s01"
    topomaps_folder = f"topomaps_reduced_{subject}"
    labels_folder = f"labels_reduced_{subject}"

    # Load data
    x_train, x_test, y_train, y_test = load_data(topomaps_folder, labels_folder, 0.2, False)

    # I am reducing the size of data set for speed purposes. For tests only
    # new_size = 200
    # x_train, y_train = reduce_size(x_train, y_train, new_size)
    # x_test, y_test = reduce_size(x_test, y_test, new_size)

    # Expand dimensions to (None, 40, 40, 1)
    # This is because VAE is currently working with 4d tensors
    x_train = expand(x_train)
    x_test = expand(x_test)

    # Print data shapes
    print("x_train shape:", x_train.shape)
    print("y_train shape:", y_train.shape)
    print("x_test shape:", x_test.shape)
    print("y_test shape:", y_test.shape)

    # Normalization between 0 and 1
    # TODO: check normalizzazione "a monte"
    x_train = normalize(x_train)
    x_test = normalize(x_test)

    # Grid search
    latent_dimension = 28
    grid = custom_grid_search(x_train, latent_dimension)

    # Refit
    history, vae = refit(grid, x_train, y_train, latent_dimension)
    save(history, subject)
    vae.save_weights(f"checkpoints/vae_{subject}", save_format='tf')

    # Questa parte serve per serializzare i pesi e verificare che a seguito del load
    # Essi siano uguali nel file analysis.py
    # dbg
    w_before = vae.get_weights()
    with open(f"w_before_{subject}.pickle", "wb") as fp:
        pickle.dump(w_before, fp)

    # plot_metric(history, "loss")
    # plot_metric(history, "reconstruction_loss")
    # plot_metric(history, "kl_loss")"""
